{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0db74b6cf253f3d1752866f1a73259b874beb65d95511e62ec1f648eb0403f7e3",
   "display_name": "Python 3.8.8 64-bit ('vision': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Import Libaries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cannot find the pretrained mobilenetv2 backbone\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-ce5cc3250614>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mMODNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMODNet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\01 D\\ML project\\Remove_Image_Background\\MODNet\\src\\models\\modnet.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, hr_channels, backbone_arch, backbone_pretrained)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackbone_pretrained\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_pretrained_ckpt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\01 D\\ML project\\Remove_Image_Background\\MODNet\\src\\models\\backbones\\wrapper.py\u001b[0m in \u001b[0;36mload_pretrained_ckpt\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cannot find the pretrained mobilenetv2 backbone'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mckpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "# import local modules\n",
    "from MODNet.src.models.modnet import MODNet\n",
    "\n",
    "modnet = torch.nn.DataParallel(MODNet())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"{{url_for('static', filename ='bgrm_result/example_1.png')}}\"\n"
     ]
    }
   ],
   "source": [
    "image = \"example_1.png\"\n",
    "\n",
    "print('''\"''' + \"{{url_for('static', filename =\" + f\"'bgrm_result/{image}'\" + \")}}\" + '''\"''' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted, ns\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "import gdown\n",
    "import torch\n",
    "import shutil \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "source": [
    "## Download U2Net, MODNet and models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1gNJXQPUBBp2mbA4q1Giz5mzv3EpxR7lq\n",
      "To: c:\\01 D\\ML project\\Remove_Image_Background\\MODNet\\pretrained\\mobilenetv2_human_seg.ckpt\n",
      "9.09MB [00:03, 2.59MB/s]\n"
     ]
    }
   ],
   "source": [
    "# if repository not already in directory, clone it\n",
    "if not os.path.exists('U-2-Net'):\n",
    "  !git clone https://github.com/xuebinqin/U-2-Net.git\n",
    "\n",
    "# if repository not already in directory, clone it\n",
    "if not os.path.exists('MODNet'):\n",
    "  !git clone https://github.com/ZHKKKe/MODNet.git\n",
    "\n",
    "# create the folder to save model\n",
    "if not os.path.exists(r'./U-2-Net/saved_models/u2net/'):\n",
    "    os.mkdir(r'./U-2-Net/saved_models/u2net/')\n",
    "\n",
    "# download U-2-net model \n",
    "if not os.path.isfile(r'./U-2-Net/saved_models/u2net/u2net.pth'):\n",
    "    url = 'https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ'\n",
    "    output = r'./U-2-Net/saved_models/u2net/u2net.pth'\n",
    "    gdown.download(url, output, quiet=False)\n",
    "\n",
    "# download modnet_mobilenetn2 model \n",
    "if not os.path.isfile(r'./MODNet/pretrained/mobilenetv2_human_seg.ckpt'):\n",
    "    url = 'https://drive.google.com/uc?id=1gNJXQPUBBp2mbA4q1Giz5mzv3EpxR7lq'\n",
    "    output = r'./MODNet/pretrained/mobilenetv2_human_seg.ckpt'\n",
    "    gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing TRIMAP\n",
    "def compute_trimap_from_alpha(name, src_dir, dst_dir, open_size=10,\n",
    "                              alpha_margin=10):\n",
    "    src_path = os.path.join(src_dir, name)\n",
    "    # if not os.path.exists(src_path):\n",
    "    #     logger.error('\"%s\" dose not exist', src_path)\n",
    "    #     return\n",
    "    print(src_path)\n",
    "\n",
    "    dst_path = os.path.join(dst_dir, name)\n",
    "    print(dst_path)\n",
    "    # if os.path.exists(dst_path):\n",
    "    #     logger.info('\"%s\" exists, Skip...', dst_path)\n",
    "    #     return\n",
    "\n",
    "    # logger.info('Trimap for \"%s\"', src_path)\n",
    "    alpha = cv2.imread(src_path, 0)\n",
    "    print(alpha)\n",
    "    assert alpha.ndim == 2\n",
    "\n",
    "    # Compute each region\n",
    "    fore = ((255 - alpha_margin) < alpha)\n",
    "    back = (alpha < alpha_margin)\n",
    "    unknown = ~(fore + back)\n",
    "    unknown = cv2.dilate(\n",
    "        unknown.astype(np.uint8),\n",
    "        cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (open_size, open_size))\n",
    "    ).astype(np.bool)\n",
    "\n",
    "    # Draw\n",
    "    trimap = np.zeros_like(alpha)\n",
    "    trimap[fore] = 255\n",
    "    trimap[unknown] = 127\n",
    "\n",
    "    cv2.imwrite(dst_path, trimap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data in correct format for MODNet training  \n",
    "def dataloader(file_location):\n",
    "    path = file_location\n",
    "    image_path = os.listdir(path + 'images')\n",
    "    gt_path = os.listdir(path + 'ground_truths')\n",
    "    trimap = os.listdir(path + 'trimaps')\n",
    "\n",
    "    # short in an order\n",
    "    image_path = natsorted(image_path, alg=ns.IGNORECASE)\n",
    "    gt_path = natsorted(gt_path, alg=ns.IGNORECASE)\n",
    "    trimap = natsorted(trimap, alg=ns.IGNORECASE)\n",
    "\n",
    "    return zip(image_path, trimap, gt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting trimap and manging storage and memory\n",
    "def get_trimap(path, image):\n",
    "\n",
    "    # main image dir should have 3 important folders images, ground_truths, trimaps\n",
    "    # (one image at a time) copy target image to ./U-2-Net/test_image\n",
    "    shutil.copy2(f'{path}images/{image}', './U-2-Net/test_data/test_images')\n",
    "\n",
    "    # generate alpha_matting\n",
    "    %cd ./U-2-Net/\n",
    "    !python u2net_test.py\n",
    "\n",
    "    # generate trimap \n",
    "    %cd ..\n",
    "    compute_trimap_from_alpha(f\"{image[:-4]}.png\", \"./U-2-Net/test_data/u2net_results/\", f'{path}trimaps/')\n",
    "\n",
    "    # delete every image file from test_image and u2net_results\n",
    "    os.remove(f'./U-2-Net/test_data/test_images/{image}')\n",
    "    \n",
    "    shutil.move(f'./U-2-Net/test_data/u2net_results/{image[:-4]}.png', f'{path}alpha_matting/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geting trimap of every image from the test and traing set\n",
    "paths = ['./dataset/training_set/', './dataset/test_set/']\n",
    "images = os.listdir(f'{path}/images')\n",
    "\n",
    "# uncomment to generete trimaps (WARNING TAKES LOT OF TIME)\n",
    "\"\"\"\n",
    "for path in paths:\n",
    "    for image in images:\n",
    "        get_trimap(path, image)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path, binary=False):\r\n",
    "  \"\"\"\r\n",
    "  Function to process image into the input format required\r\n",
    "  for model\r\n",
    "  \"\"\" \r\n",
    "  # read image\r\n",
    "  im = Image.open(image_path)\r\n",
    "\r\n",
    "  # define image to tensor transform\r\n",
    "  im_transform = transforms.Compose(\r\n",
    "      [\r\n",
    "          transforms.ToTensor(),\r\n",
    "          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\r\n",
    "      ])\r\n",
    "\r\n",
    "  gt_transform = transforms.Compose(\r\n",
    "      [\r\n",
    "          transforms.ToTensor(),\r\n",
    "          transforms.Normalize((0.5), (0.5))\r\n",
    "      ])\r\n",
    "\r\n",
    "  # unify image channels to 3\r\n",
    "  if binary == False:\r\n",
    "      \r\n",
    "    im = np.asarray(im)\r\n",
    "    if len(im.shape) == 2:\r\n",
    "        im = im[:, :, None]\r\n",
    "    if im.shape[2] == 1:\r\n",
    "        im = np.repeat(im, 3, axis=2)\r\n",
    "    elif im.shape[2] == 4:\r\n",
    "        im = im[:, :, 0:3]\r\n",
    "        \r\n",
    "    # convert image to PyTorch tensor\r\n",
    "    im = Image.fromarray(im)\r\n",
    "    im = im_transform(im).cuda()\r\n",
    "\r\n",
    "  else:\r\n",
    "    im = im.convert('1') \r\n",
    "    im = np.asarray(im)\r\n",
    "\r\n",
    "    # convert image to PyTorch tensor\r\n",
    "    im = Image.fromarray(im)\r\n",
    "    im = gt_transform(im).cuda()\r\n",
    "\r\n",
    "  # add mini-batch dim\r\n",
    "  im = im[None, :, :, :]\r\n",
    "\r\n",
    "  # resize image for input\r\n",
    "  im_b, im_c, im_h, im_w = im.shape\r\n",
    "\r\n",
    "  if max(im_h, im_w) < ref_size or min(im_h, im_w) > ref_size:\r\n",
    "      if im_w >= im_h:\r\n",
    "          im_rh = ref_size\r\n",
    "          im_rw = int(im_w / im_h * ref_size)\r\n",
    "      elif im_w < im_h:\r\n",
    "          im_rw = ref_size\r\n",
    "          im_rh = int(im_h / im_w * ref_size)\r\n",
    "  else:\r\n",
    "      im_rh = im_h\r\n",
    "      im_rw = im_w\r\n",
    "\r\n",
    "  im_rw = im_rw - im_rw % 32\r\n",
    "  im_rh = im_rh - im_rh % 32\r\n",
    "  im = F.interpolate(im, size=(im_rh, im_rw), mode='area')\r\n",
    "\r\n",
    "  return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(image_dir_list: list) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Function to get a dataframe with the different paths for each image\n",
    "  \"\"\"\n",
    "  # create empty list\n",
    "  image_df_list = [] # will contain a dataframe for each directory\n",
    "\n",
    "  # create dataframes with list of image paths per directory\n",
    "  for image_dir in image_dir_list:\n",
    "    image_path_list = glob.glob(image_dir + os.sep + '*')\n",
    "    image_df = pd.DataFrame(image_path_list, columns=[image_dir])\n",
    "\n",
    "    # create a filename column by extracting the image filename without extension\n",
    "    image_df[\"filename\"] = image_df[image_dir].str.split(os.sep).str[-1].str.split(\".\").str[0]\n",
    "\n",
    "    # append dataframe to dataframe list\n",
    "    image_df_list.append(image_df)\n",
    "    print(image_df.shape)\n",
    "\n",
    "  # merge dataframes on the image filename\n",
    "  df = pd.merge(image_df_list[0], image_df_list[1], on=\"filename\")\n",
    "  print(df.shape)\n",
    "  df = pd.merge(df, image_df_list[2], on=\"filename\")\n",
    "  print(df.shape)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(image_paths_df, batch_size):\n",
    "  \"\"\"\n",
    "  Function to load the model with the paths of:\n",
    "  * original images\n",
    "  * trimaps \n",
    "  * ground truth images\n",
    "  \"\"\"\n",
    "  \n",
    "  np.array_split(df, 16)\n",
    "\n",
    "  paths_list = []\n",
    "  print(image_paths_df.columns)\n",
    "  for column_name in image_paths_df.columns:\n",
    "    if column_name != \"filename\":\n",
    "      print(column_name)\n",
    "      paths = image_paths_df[column_name].to_list()\n",
    "      paths_list.append(paths)\n",
    "    \n",
    "  image_list, trimap_list, gt_path_list = paths_list[0], paths_list[1], paths_list[2]\n",
    "\n",
    "  return zip(image_list, trimap_list, gt_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}