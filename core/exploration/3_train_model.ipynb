{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_train_model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.8.10"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"36s2EXUbfq9Y"},"source":["# Train MODNet model\n","\n","This is the 3rd step in our workflow to remove the background from an image:\n","\n","1. Use U-2-Net pre-trained model to generate a first alpha matte\n","2. **Use the U-2-Net alpha matte to generate a trimap\n","3. **Train MODNet model with the original image, the trimap and ground truth image from DUTS dataset** (the current colab notebook)\n","\n","## Sources:\n","* \n"]},{"cell_type":"markdown","metadata":{"id":"rxKC0bw0gXPh"},"source":["# Import"]},{"cell_type":"code","metadata":{"id":"Kf0-7Hf9fbyd"},"source":["# import modules to handle files\n","import os\n","import glob\n","import shutil\n","# from google.colab import drive\n","from PIL import Image\n","\n","# import modules to train models\n","import torch\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","import pandas as pd\n","import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KpzH1clzjB2Y"},"source":["# Mount Google Drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UwB3pLH7jBCC","executionInfo":{"status":"ok","timestamp":1623327538007,"user_tz":-120,"elapsed":27984,"user":{"displayName":"joren vervoort","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipKq-eIOSRnOHXkpNjG6cyQ0oZU6rjZZ4YiVB1=s64","userId":"12855967750163220688"}},"outputId":"ad737626-a125-450a-c7ea-2721296eac51"},"source":["# drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P6Fnw1MJA7pH"},"source":["# Clone MODNet repo & download pre-trained model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zC24MhB8moin","executionInfo":{"status":"ok","timestamp":1623327554627,"user_tz":-120,"elapsed":3555,"user":{"displayName":"joren vervoort","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipKq-eIOSRnOHXkpNjG6cyQ0oZU6rjZZ4YiVB1=s64","userId":"12855967750163220688"}},"outputId":"250d01d2-5fb2-4aeb-9126-2d161a1d355c"},"source":["# # clone the repository\n","# %cd /content\n","if not os.path.exists('MODNet'):\n","  !git clone https://github.com/ZHKKKe/MODNet\n","%cd MODNet/"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content'\n","/Users/derrickvanfrausum/BeCode_AI/git-repos/Remove_Image_Background/core/exploration\n","Cloning into 'MODNet'...\n","remote: Enumerating objects: 213, done.\u001b[K\n","remote: Counting objects: 100% (7/7), done.\u001b[K\n","remote: Compressing objects: 100% (7/7), done.\u001b[K\n","remote: Total 213 (delta 1), reused 0 (delta 0), pack-reused 206\u001b[K\n","Receiving objects: 100% (213/213), 37.62 MiB | 3.11 MiB/s, done.\n","Resolving deltas: 100% (62/62), done.\n","/Users/derrickvanfrausum/BeCode_AI/git-repos/Remove_Image_Background/core/exploration/MODNet\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"tuqR7lrDmsGs","executionInfo":{"status":"ok","timestamp":1623327559374,"user_tz":-120,"elapsed":4750,"user":{"displayName":"joren vervoort","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipKq-eIOSRnOHXkpNjG6cyQ0oZU6rjZZ4YiVB1=s64","userId":"12855967750163220688"}},"outputId":"89bccf07-19db-4732-f167-d56dbcf67350"},"source":["# # copy pre-trained model to directory\n","# current_path = \"/content/drive/MyDrive/BeCode/Projects/Faktion/exploration/pretrained_models/mobilenetv2_human_seg.ckpt\"\n","# dst_path = \"/content/MODNet/pretrained/mobilenetv2_human_seg.ckpt\"\n","# shutil.copy(current_path, dst_path)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/MODNet/pretrained/mobilenetv2_human_seg.ckpt'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/Users/derrickvanfrausum/BeCode_AI/git-repos/Remove_Image_Background/core/exploration/MODNet/pretrained/mobilenetv2_human_seg.ckpt'"]},"metadata":{},"execution_count":4}],"source":["# copy pre-trained model to directory\n","current_path = \"/Users/derrickvanfrausum/BeCode_AI/git-repos/Remove_Image_Background/core/assets/pretrained_models/mobilenetv2_human_seg.ckpt\"\n","dst_path = \"/Users/derrickvanfrausum/BeCode_AI/git-repos/Remove_Image_Background/core/exploration/MODNet/pretrained/mobilenetv2_human_seg.ckpt\"\n","shutil.move(current_path, dst_path)"]},{"cell_type":"markdown","metadata":{"id":"np75gwoFmywh"},"source":["# Train model to predict alpha matte"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["global device\n","\n","# define device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"markdown","metadata":{"id":"JkfJ2PZdm_wK"},"source":["## Functions"]},{"cell_type":"code","metadata":{"id":"Q_qfGK2qoH5X"},"source":["def process_image(image_path, binary=False):\n","  \"\"\"\n","  Function to process image into the input format required\n","  for model\n","  \"\"\"\n","  \n","  # read image\n","  im = Image.open(image_path)\n","\n","\n","  # define image to tensor transform\n","  im_transform = transforms.Compose(\n","      [\n","          transforms.ToTensor(),\n","          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","      ]\n","  )\n","\n","  gt_transform = transforms.Compose(\n","      [\n","          transforms.ToTensor(),\n","          transforms.Normalize((0.5), (0.5))\n","      ]\n","  )\n","\n","  # unify image channels to 3\n","  if binary == False:\n","      \n","    im = np.asarray(im)\n","    if len(im.shape) == 2:\n","        im = im[:, :, None]\n","    if im.shape[2] == 1:\n","        im = np.repeat(im, 3, axis=2)\n","    elif im.shape[2] == 4:\n","        im = im[:, :, 0:3]\n","        \n","    # convert image to PyTorch tensor\n","    im = Image.fromarray(im)\n","    im = im_transform(im).to(device)\n","\n","  else:\n","    im = im.convert('1') \n","    im = np.asarray(im)\n","\n","    # convert image to PyTorch tensor\n","    im = Image.fromarray(im)\n","    im = gt_transform(im).to(device)\n","\n","\n","  # add mini-batch dim\n","  im = im[None, :, :, :]\n","\n","  # resize image for input\n","  im_b, im_c, im_h, im_w = im.shape\n","\n","  if max(im_h, im_w) < ref_size or min(im_h, im_w) > ref_size:\n","      if im_w >= im_h:\n","          im_rh = ref_size\n","          im_rw = int(im_w / im_h * ref_size)\n","      elif im_w < im_h:\n","          im_rw = ref_size\n","          im_rh = int(im_h / im_w * ref_size)\n","  else:\n","      im_rh = im_h\n","      im_rw = im_w\n","\n","  im_rw = im_rw - im_rw % 32\n","  im_rh = im_rh - im_rh % 32\n","  im = F.interpolate(im, size=(im_rh, im_rw), mode='area')\n","\n","  return im"],"execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"id":"p7NKOhAYm-KC"},"source":["def get_image_paths(image_dir_list: list) -> pd.DataFrame:\n","  \"\"\"\n","  Function to get a dataframe with the different paths for each image\n","  \"\"\"\n","  # create empty list\n","  image_df_list = [] # will contain a dataframe for each directory\n","\n","  # create dataframes with list of image paths per directory\n","  for image_dir in image_dir_list:\n","    image_path_list = glob.glob(image_dir + os.sep + '*')\n","    image_df = pd.DataFrame(image_path_list, columns=[image_dir])\n","\n","    # create a filename column by extracting the image filename without extension\n","    image_df[\"filename\"] = image_df[image_dir].str.split(os.sep).str[-1].str.split(\".\").str[0]\n","\n","    # append dataframe to dataframe list\n","    image_df_list.append(image_df)\n","    print(image_df.shape)\n","\n","  # merge dataframes on the image filename\n","  df = pd.merge(image_df_list[0], image_df_list[1], on=\"filename\")\n","  print(df.shape)\n","  df = pd.merge(df, image_df_list[2], on=\"filename\")\n","  print(df.shape)\n","\n","  return df"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5j6ZSUEzx93"},"source":["def dataloader(image_paths_df):\n","  \"\"\"\n","  Function to load the model with the paths of:\n","  * original images\n","  * trimaps \n","  * ground truth images\n","  \"\"\"\n","\n","  paths_list = []\n","  print(image_paths_df.columns)\n","  for column_name in image_paths_df.columns:\n","    if column_name != \"filename\":\n","      print(column_name)\n","      paths = image_paths_df[column_name].to_list()\n","      paths_list.append(paths)\n","    \n","  image_list, trimap_list, gt_path_list = paths_list[0], paths_list[1], paths_list[2]\n","\n","  return zip(image_list, trimap_list, gt_path_list)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YLH5_kVtnIss"},"source":["## Train model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c31QtnXWwsy4","executionInfo":{"status":"ok","timestamp":1623327589551,"user_tz":-120,"elapsed":11446,"user":{"displayName":"joren vervoort","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipKq-eIOSRnOHXkpNjG6cyQ0oZU6rjZZ4YiVB1=s64","userId":"12855967750163220688"}},"outputId":"39f4edcf-531c-4b86-9a24-c66380cf04d0"},"source":["## copy modified modnet script that trains the model\n","# current_path = \"/content/drive/MyDrive/BeCode/Projects/Faktion/exploration/modnet_trainer_modified.py\"\n","# dst_path = \"/content/MODNet/src/modnet_trainer_modified.py\"\n","# shutil.copy(current_path, dst_path)\n","\n","\n","# cd to repository\n","%cd /Users/derrickvanfrausum/BeCode_AI/git-repos/Remove_Image_Background/core/exploration/MODNet\n","\n","# import local modules\n","from MODNet.src.models.modnet import MODNet\n","from MODNet.src.trainer_modified import supervised_training_iter"],"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["/Users/derrickvanfrausum/BeCode_AI/git-repos/Remove_Image_Background/core/exploration/MODNet\n"]}]},{"cell_type":"code","metadata":{"id":"H51y2_xsEWN2"},"source":["# importing and activating TensorBoard\n","\n","from torch.utils.tensorboard import SummaryWriter\n","writer = SummaryWriter()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X8EQTBeVwBsD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623327788399,"user_tz":-120,"elapsed":197421,"user":{"displayName":"joren vervoort","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipKq-eIOSRnOHXkpNjG6cyQ0oZU6rjZZ4YiVB1=s64","userId":"12855967750163220688"}},"outputId":"867a0073-9948-450d-d9b2-573f79d67570"},"source":["# get list of image directory paths\n","src_dir = \"/Users/derrickvanfrausum/BeCode_AI/git-repos/Remove_Image_Background/core/assets/dataset/DUTS/DUTS-TR\"\n","image_dir_list = [\"DUTS-TR-Image\", \"DUTS-TR-Trimap\", \"DUTS-TR-Mask\"]\n","\n","## convert paths into absolute paths\n","image_dir_list = [os.path.join(src_dir, image_dir) for image_dir in image_dir_list]\n","\n","df = get_image_paths(image_dir_list)\n","\n","# save df to csv\n","df.to_csv(\"/Users/derrickvanfrausum/BeCode_AI/git-repos/Remove_Image_Background/core/assets/dataset/DUTS/image_paths.csv\", index=False)"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["(10553, 2)\n(10553, 2)\n(10553, 2)\n(10553, 3)\n(10553, 4)\n"]}]},{"cell_type":"code","metadata":{"id":"pyyvj5ttIgJg"},"source":["# # load csv into dataframe\n","# df = pd.read_csv(\"/Users/derrickvanfrausum/BeCode_AI/git-repos/Remove_Image_Background/core/assets/dataset/DUTS/image_paths.csv\")\n","\n","# hyperparameters\n","batch_size = 16\n","lr = 0.001      # learn rate\n","epochs = 40     # total epochs\n","\n","# split dataframe into batches\n","batches = np.array_split(df, batch_size)\n","\n","# define device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"execution_count":21,"outputs":[]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["import re\n","from typing import Optional"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["def get_latest_model_chekpoint_path(saved_models_dir) -> Optional[str]:\n","    \"\"\"\n","    Function to get the path of the latest saved \n","    model checkpoint\n","    \n","    returns: None or path in string\n","    \"\"\"\n","    model_checkpoint_path_list = glob.glob(saved_models_dir + os.sep + 'model_checkpoint_epoch' + '*')\n","    if len(model_checkpoint_path_list) > 0:\n","        model_idx_list = []\n","        for filepath in model_checkpoint_path_list:\n","            model_idx = int(re.findall(r'\\d+', filepath)[0])\n","            model_idx_list.append(model_idx)\n","\n","        # convert list to array & get latest idx\n","        latest_idx = np.argmax(np.array(model_idx_list))\n","        latest_checkpoint = model_checkpoint_path_list[latest_idx]\n","        print(\"latest model checkpoint: \", latest_checkpoint)\n","        return latest_checkpoint"]},{"cell_type":"code","metadata":{"id":"2YfRp-B4nrMV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"49590c04-c700-4a4f-a930-dcee069fd644","tags":[]},"source":["# initialize model\n","modnet = torch.nn.DataParallel(MODNet()).to(device)\n","\n","# model index: updated if training starts from saved models\n","model_idx = -1\n","\n","# load latest saved trained model if available\n","saved_models_dir = \"/Users/derrickvanfrausum/BeCode_AI/git-repos/Remove_Image_Background/core/assets/saved_models\"\n","latest_model_chekpoint_path = get_latest_model_chekpoint_path(saved_models_dir)\n","\n","if latest_model_chekpoint_path:\n","  # load model's state_dict\n","  state_dict = torch.load(\n","    latest_model_chekpoint_path,\n","    map_location=torch.device(device) # map to device\n","      )\n","  # print(state_dict.keys())\n","\n","  # load state_dict into the network (works only if model architecture is the same as checkpoint architecture)\n","  modnet.load_state_dict(state_dict)\n","\n","  # get path of loss metrics checkpoint\n","  model_idx = re.findall(r'\\d+', path)[0]\n","  latest_loss_chekpoint_path = os.path.join(\n","    saved_models_dir,\n","    f\"loss_checkpoint_epoch_{model_idx}.pth\")\n","  print(latest_loss_chekpoint_path)\n","\n","  # load loss metrics checkpoints\n","  loss_dict = torch.load(\n","    latest_loss_chekpoint_path,\n","    map_location=torch.device(device) # map to device\n","      )\n","  # print(loss_dict.keys())\n","\n","else:\n","  # create empty dict\n","  loss_dict = {}\n","  loss_dict[\"semantic_loss\"] = []\n","  loss_dict[\"detail_loss\"] = []\n","  loss_dict[\"matte_loss\"] = []\n","\n","# initialize optimizer\n","optimizer = torch.optim.SGD(modnet.parameters(), lr=lr, momentum=0.9)\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(0.25 * epochs), gamma=0.1)\n","\n","# define hyper-parameters\n","ref_size = 512\n","\n","# training loop starting from latest saved epoch\n","for epoch in range(int(model_idx) + 1, epochs + 1):\n","  print(\"\\n\\n*****************************************\")\n","  print(\"epoch:\",epoch)\n","  print(\"*****************************************\")\n","\n","  for batch_df in batches:\n","    print(type(batch_df))\n","    for idx, (image_path, trimap_path, gt_matte_path) in enumerate(dataloader(batch_df)):\n","      print(idx, image_path, trimap_path, gt_matte_path)\n","      image = process_image(image_path)\n","      trimap = process_image(trimap_path)\n","      gt_matte = process_image(gt_matte_path, True)\n","\n","      semantic_loss, detail_loss, matte_loss = \\\n","      supervised_training_iter(modnet, optimizer, image, trimap, gt_matte)\n","\n","    lr_scheduler.step()\n","\n","  # save model's state_dict\n","  torch.save(\n","    modnet.state_dict(),\n","    os.path.join(\n","      saved_models_dir,\n","      f\"model_checkpoint_epoch_{epoch}.pth\"\n","      )\n","      )\n","\n","  # updates loss dict\n","  # --------------------------------------------------------------------\n","  # WE NEED TO SAVE THE LOSSES AFTER EVERY EPOCH TO PLOT THEM IN A GRAPH\n","  # --------------------------------------------------------------------\n","  loss_dict[\"semantic_loss\"].append(semantic_loss)\n","  loss_dict[\"detail_loss\"].append(detail_loss)\n","  loss_dict[\"matte_loss\"].append(matte_loss)\n","\n","  # --------------------------------------------------------------------\n","  # Saving the loss values for tensorboard\n","  # --------------------------------------------------------------------\n","  writer.add_scalar(\"semantic_loss/train\", semantic_loss, epoch)\n","  writer.add_scalar(\"detail_loss/train\", detail_loss, epoch)\n","  writer.add_scalar(\"matte_loss/train\", matte_loss, epoch)\n","\n","  # save loss metrics\n","  torch.save(\n","    loss_dict,\n","    os.path.join(\n","      saved_models_dir,\n","      f\"loss_checkpoint_epoch_{epoch}.pth\"\n","      )\n","      )\n","\n","  print(\"\\n\\n---------------------------------------\")\n","  print(\"saved losses of epoch\",epoch)\n","  print(\"semantic_loss:\",semantic_loss, \"detail_loss:\",detail_loss,\"matte_loss:\",matte_loss)\n","  print(\"---------------------------------------\")\n","# --------------------------------------------------------------------\n","# conclude tensorboard\n","# --------------------------------------------------------------------\n","writer.flush()\n","\n","#writer.close() \"if you don't need it anymore\" - quote of Derrick\n"],"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["latest model checkpoint:  /Users/derrickvanfrausum/BeCode_AI/git-repos/Remove_Image_Background/core/assets/saved_models/model_checkpoint_epoch_1.pth\n/Users/derrickvanfrausum/BeCode_AI/git-repos/Remove_Image_Background/core/assets/saved_models/loss_checkpoint_epoch_1.pth\n"]},{"output_type":"error","ename":"TypeError","evalue":"can only concatenate str (not \"int\") to str","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-75-2d1d6546d10e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# training loop starting from latest saved epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n*****************************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"]}]},{"cell_type":"markdown","metadata":{"id":"gAwSBjfVnv1B"},"source":["## Save & load model\n","Parameters for PyTorch networks stored in the model's `state_dict`\n","\n","[Source](https://colab.research.google.com/github/agungsantoso/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/Part%206%20-%20Saving%20and%20Loading%20Models.ipynb#scrollTo=lBqSgQCNpCX4)"]},{"cell_type":"code","metadata":{"id":"R6AFZQCCn1BV","colab":{"base_uri":"https://localhost:8080/","height":181},"executionInfo":{"status":"error","timestamp":1623179217824,"user_tz":-120,"elapsed":603,"user":{"displayName":"Van Frausum Derrick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPlc6uDDSsjHzaIeMPdr-Jw14nRfyQhGuaw0TDJw=s64","userId":"04282691266898005200"}},"outputId":"eb83d26f-5731-4b16-960c-5bde4cb150b8"},"source":["print(\"The trained model: \\n\\n\", modnet, '\\n')\n","print(\"The state_dict keys: \\n\\n\", modnet.state_dict().keys())"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-55161cf951d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The trained model: \\n\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The state_dict keys: \\n\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'modnet' is not defined"]}]},{"cell_type":"code","metadata":{"id":"KUyhofC-n9jf"},"source":["from google.colab import files\n","\n","# # create dictionary with all information necessary to rebuild the model.\n","# checkpoint = {'input_size': 784,\n","#               'output_size': 10,\n","#               'hidden_layers': [each.out_features for each in model.hidden_layers],\n","#               'state_dict': modnet.state_dict()}\n","\n","# # save model's architecture and state_dict\n","# torch.save(checkpoint, 'model_checkpoint.pth')\n","\n","# save model's state_dict\n","torch.save(modnet.state_dict(), 'model_checkpoint.pth')\n","\n","# download checkpoint file\n","files.download('model_checkpoint.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a_GJy_qOn_Ip"},"source":["# load model's state_dict\n","state_dict = torch.load('model_checkpoint.pth')\n","print(state_dict.keys())\n","\n","# load state_dict into the network (works only if model architecture is the same as checkpoint architecture)\n","modnet.load_state_dict(state_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iXzIBHqkoB6f"},"source":["# def load_checkpoint(filepath):\n","#   \"\"\"\n","#   Function to load saved model and rebuild it.\n","#   \"\"\"\n","#     checkpoint = torch.load(filepath)\n","#     model = modnet.Network(checkpoint['input_size'],\n","#                              checkpoint['output_size'],\n","#                              checkpoint['hidden_layers'])\n","#     model.load_state_dict(checkpoint['state_dict'])\n","    \n","#     return model\n","model = load_checkpoint('checkpoint.pth')\n","print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9y4-mP2ioM-Z"},"source":["# Predict alpha matte with trained model"]},{"cell_type":"code","metadata":{"id":"tIflb7TooSHp"},"source":["image_path = \"/content/U-2-Net/test_data/test_images/0002-01.jpg\"\n","trimap_path = \"/content/drive/MyDrive/trimaps/0002-01.png\"\n","im = process_image(image_path)\n","trimap = process_image(trimap_path)\n","\n","_, _, matte = modnet(im, True)\n","\n","# resize and save matte\n","matte = F.interpolate(matte, size=(im_h, im_w), mode='area')\n","matte = matte[0][0].data.cpu().numpy()\n","matte_name = 'test_b_v2.png'\n","\n","Image.fromarray(((matte * 255).astype('uint8')), mode='L').save(os.path.join(\"/content\", matte_name))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NbW9KXLJoUkn"},"source":["# Performance metrics"]},{"cell_type":"markdown","metadata":{"id":"l5NtNoakpT3I"},"source":["## Functions"]},{"cell_type":"code","metadata":{"id":"yRNkRVbtoTOb"},"source":["# compute the MSE error given a prediction, a ground truth and a trimap.\n","# pred: the predicted alpha matte\n","# target: the ground truth alpha matte\n","# trimap: the given trimap\n","#\n","def compute_mse(pred, alpha, trimap):\n","    num_pixels = float((trimap == 127).sum())\n","    return ((pred - alpha) ** 2).sum() / num_pixels\n","\n","\n","# compute the SAD error given a prediction and a ground truth.\n","#\n","def compute_sad(pred, alpha):\n","    diff = np.abs(pred - alpha)\n","    return np.sum(diff) / 1000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dsRGRmWPpYBL"},"source":["# import cv2\n","\n","gt_matte = cv2.imread(\"/content/drive/MyDrive/Faktion/exploration/dataset/ground_truths/ILSVRC2012_test_00000018.png\")\n","trimap = cv2.imread(\"/content/drive/MyDrive/Faktion/exploration/dataset/trimaps/ILSVRC2012_test_00000018.png\")\n","pred = cv2.imread(\"/content/test.png\")\n","\n","print(compute_mse(pred, gt_matte, trimap))\n","print(compute_sad(pred, gt_matte))"],"execution_count":null,"outputs":[]}]}